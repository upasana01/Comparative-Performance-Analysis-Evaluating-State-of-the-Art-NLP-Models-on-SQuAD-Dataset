{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook demonstrates the implementation of a question-answering system using the T5ForConditionalGeneration model from the Hugging Face Transformers library. We'll process the SQuAD (Stanford Question Answering Dataset) to train and validate our model, and evaluate its performance using various metrics such as F1, BLEU, and ROUGE scores."
      ],
      "metadata": {
        "id": "k93RQtneXBVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Imports"
      ],
      "metadata": {
        "id": "YXm713TbFufM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BIt4cZb_FY3i"
      },
      "outputs": [],
      "source": [
        " # Import necessary libraries\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Load Model and Tokenizer\n"
      ],
      "metadata": {
        "id": "3ohKzqcMFzja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Load T5 large model and tokenizer\n",
        "model_name = 't5-small'\n",
        "# model_name = 't5-base'\n",
        "# model_name = 't5-large'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMXDAFHFFxuc",
        "outputId": "25f3b019-9962-4432-b7ba-174b0adde3f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load and Prepare Datasets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fv9nRPpTF6Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Load SQuAD dataset and select a subset for training and validation\n",
        "dataset = load_dataset('squad')\n",
        "train_dataset = dataset['train'].select(range(10000))\n",
        "validation_dataset = dataset['validation'].select(range(1000))\n",
        "\n",
        "# Convert datasets to lists for easier processing\n",
        "train_dataset_list = [item for item in train_dataset]\n",
        "validation_dataset_list = [item for item in validation_dataset]\n"
      ],
      "metadata": {
        "id": "cr_BjuZGF3jx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Load Metrics"
      ],
      "metadata": {
        "id": "9FvHZIbDF_ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the metric for evaluating the model's performance\n",
        "metric = load_metric('squad')\n"
      ],
      "metadata": {
        "id": "bBXltc1xF96y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Define Functions for Model Evaluation\n"
      ],
      "metadata": {
        "id": "l8n90ivWGBSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Define function to generate answers from the model and evaluate them\n",
        "def generate_answer_and_evaluate(dataset_list):\n",
        "    # Initialize containers for results\n",
        "    predictions = []\n",
        "    plain_text_predictions = []\n",
        "    plain_text_references = []\n",
        "    references = []\n",
        "\n",
        "    # Process each data item\n",
        "    for item in dataset_list:\n",
        "        question = item['question']\n",
        "        context = item['context']\n",
        "        input_text = f\"question: {question} context: {context}\"\n",
        "        input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "        output_ids = model.generate(input_ids, max_length=200, num_beams=5, early_stopping=True)\n",
        "        predicted_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Collect results for F1, Exact Match, BLEU, and ROUGE\n",
        "        predictions.append({'id': item['id'], 'prediction_text': predicted_answer})\n",
        "        references.append({'id': item['id'], 'answers': item['answers']})\n",
        "        plain_text_predictions.append(predicted_answer)\n",
        "        plain_text_references.append(item['answers']['text'][0])\n",
        "\n",
        "    # Compute and return evaluation metrics\n",
        "    squad_results = metric.compute(predictions=predictions, references=references)\n",
        "    exact_matches = sum(1 for ref, pred in zip(plain_text_references, plain_text_predictions) if ref == pred)\n",
        "    accuracy = exact_matches / len(plain_text_references) * 100\n",
        "    return squad_results, plain_text_predictions, plain_text_references, accuracy\n",
        "\n",
        "# Functions to calculate BLEU and ROUGE scores\n",
        "def calculate_bleu(references, hypotheses):\n",
        "    list_of_references = [[ref.split()] for ref in references]\n",
        "    hypotheses_formatted = [hyp.split() for hyp in hypotheses]\n",
        "    return corpus_bleu(list_of_references, hypotheses_formatted)\n",
        "\n",
        "def calculate_rouge(references, hypotheses):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {key: 0.0 for key in ['rouge1', 'rougeL']}\n",
        "    for ref, hyp in zip(references, hypotheses):\n",
        "        scores = scorer.score(ref, hyp)\n",
        "        for key in rouge_scores:\n",
        "            rouge_scores[key] += scores[key].fmeasure\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] /= len(hypotheses)\n",
        "    return rouge_scores\n"
      ],
      "metadata": {
        "id": "zNMwwPR0GBzW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Run Evaluation and Display Results\n"
      ],
      "metadata": {
        "id": "48HCOnqDSKTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute model evaluation on the validation dataset\n",
        "squad_results, plain_text_predictions, plain_text_references, accuracy = generate_answer_and_evaluate(validation_dataset_list)\n",
        "\n",
        "# Calculate and display BLEU and ROUGE scores\n",
        "bleu_score = calculate_bleu(plain_text_references, plain_text_predictions)\n",
        "rouge_scores = calculate_rouge(plain_text_references, plain_text_predictions)\n",
        "\n",
        "# Output all evaluation results\n",
        "print(f\"SQuAD Evaluation Results: {squad_results}\")\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "print(f\"ROUGE Scores: {rouge_scores}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OEyZSEHSLuf",
        "outputId": "922787e9-8c88-4982-e671-c51080d08478"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQuAD Evaluation Results: {'exact_match': 76.4, 'f1': 83.8792542131071}\n",
            "BLEU Score: 0.2900363598103316\n",
            "ROUGE Scores: {'rouge1': 0.7989091516670457, 'rougeL': 0.7986014593593532}\n",
            "Accuracy: 64.00%\n"
          ]
        }
      ]
    }
  ]
}